# multilayer_perceptron ğŸ§®

![image](https://i.imgur.com/18CCG1y.png)

## ğŸ“ Description

Welcome to my project of a MLP (Multilayer Perceptron) library developed from scratch, similar to [Keras](https://keras.io/api/). This library allows you to create, train, and evaluate multilayer perceptron models.

For this project, I am using predictive data to diagnose cancer in cells, with M representing malignant and B representing benign

## ğŸ“¦ Features

### Optimizers

Optimizers play a crucial role by helping to minimize the loss function and find the optimal values for the model's parameters. They guide the learning process by adjusting the parameters iteratively based on the gradients, leading to improved accuracy and faster convergence during training.

Test for

- [x] Adam
- [x] Stochastic Gradient Descent
- [x] Stochastic Gradient Descent with Nesterov Momentum
- [x] RMSprop

### Activation Functions

- [x] Linear
- [x] ReLU
- [x] Leaky ReLU
- [x] Sigmoid
- [x] Softmax
- [x] Softplus
- [x] Softsign
- [x] Tanh
- [x] Exponential

### Loss Functions

- [x] Binary Cross Entropy
- [x] Mean Squared Error

### Initializers

- [x] Random Normal
- [x] Random Uniform
- [x] Truncated Normal
- [x] Zeros
- [x] Ones
- [x] Glorot Normal
- [x] Glorot Uniform
- [x] He Normal
- [x] He Uniform
- [x] Identity

### Callbacks

- [x] Early Stopping
- [x] Model Checkpoint

### Metrics

- [x] Accuracy
- [x] Binary Accuracy
- [x] Precision
- [x] Recall
- [x] Mean Squared Error

## ğŸ“Œ TODO

- [ ] End regularization

## ğŸ‘¨â€ğŸ’» Author

- [@tsannie](https://github.com/tsannie)
